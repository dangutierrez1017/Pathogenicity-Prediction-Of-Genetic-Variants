{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y cupy-cuda11x cupy-cuda12x cupy\n",
        "!nvidia-smi\n",
        "!pip install cupy-cuda11x\n",
        "!apt-get install -y nvidia-cuda-toolkit\n",
        "!pip install cudf-cu11 dask-cudf-cu11 --extra-index-url=https://pypi.ngc.nvidia.com"
      ],
      "metadata": {
        "id": "i3EUEZw6Vz8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "import dask_cudf\n",
        "from dask.distributed import Client, LocalCluster\n",
        "from sklearn.impute import KNNImputer\n",
        "import cudf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "# Initialize a Dask LocalCluster to manage computations\n",
        "cluster = LocalCluster(\n",
        "    n_workers=2,  # Use two workers to maximize CPU and RAM usage\n",
        "    threads_per_worker=2,  # 2 threads per worker\n",
        "    memory_limit=\"6GB\",  # Split memory between workers\n",
        "    processes=True,  # Enable multiprocessing\n",
        ")\n",
        "client = Client(cluster)\n",
        "\n",
        "# Example usage\n",
        "filepath = '/content/drive/MyDrive/filtered_dbnsfp.csv'\n",
        "output_filepath = 'dbnsfp_eda_results.txt'\n",
        "\n",
        "# Missing Values list\n",
        "na_list = [\n",
        "    '#chr', 'pos(1-based)', 'ref', 'alt',\n",
        "    'SIFT_converted_rankscore', 'SIFT4G_converted_rankscore', 'Polyphen2_HDIV_rankscore',\n",
        "    '.', ''  # Shortened for clarity\n",
        "]\n",
        "\n",
        "dtypedict = {'#chr': 'object', 'pos(1-based)': 'object', 'ref': 'object', 'alt': 'object'}\n",
        "\n",
        "# Load dbNSFP into Dask-cuDF (GPU-accelerated)\n",
        "dbnsfp_data = dask_cudf.read_csv(filepath, na_values=na_list, dtype=dtypedict)\n",
        "\n",
        "# Missing Value Analysis\n",
        "rows, cols = dbnsfp_data.shape\n",
        "with open(output_filepath, \"w\") as f:\n",
        "\n",
        "    missing_values = dbnsfp_data.isnull().sum().compute()\n",
        "\n",
        "    # Convert missing_values to Pandas Series to iterate over it\n",
        "    missing_values = missing_values.to_pandas()\n",
        "\n",
        "    # Now you can iterate over the missing_values\n",
        "    with open(output_filepath, 'w') as f:\n",
        "        f.write(\"Missing values per feature (sorted):\\n\")\n",
        "        for col, count in missing_values.items():\n",
        "            f.write(f\"{col}: {count}\\n\")\n",
        "\n",
        "\n",
        "    # Compute the threshold for dropping columns with more than 80% missing values\n",
        "    threshold = int(0.8 * dbnsfp_data.shape[0].compute())  # Compute the number of rows\n",
        "\n",
        "    # Drop columns with more than 80% missing values\n",
        "    dbnsfp_data = dbnsfp_data.loc[:, dbnsfp_data.isnull().sum(axis=0) < threshold]\n",
        "\n",
        "    # Now, dbnsfp_data will only include columns with fewer than 80% missing values\n",
        "\n",
        "\n",
        "    # Imputation for numeric columns with <20% missing\n",
        "    numeric_cols = dbnsfp_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    dbnsfp_data[numeric_cols] = imputer.fit_transform(dbnsfp_data[numeric_cols].to_pandas())\n",
        "    f.write(f\"\\nImputed columns: {numeric_cols}\\n\")\n",
        "\n",
        "    # Save cleaned data\n",
        "    dbnsfp_data.to_csv('/content/cleaned_dbnsfp.csv', single_file=True)\n",
        "\n",
        "# GPU-specific operations\n",
        "client.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "KfCZxwDcYycc",
        "outputId": "d725d000-1e9a-44ab-d066-bb03d11a83df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
            "Perhaps you already have a cluster running?\n",
            "Hosting the HTTP server on port 39693 instead\n",
            "  warnings.warn(\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:33717\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:39693/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:45857'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:44769'\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:44403', name: 0, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:44403\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:51046\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:37685', name: 1, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:37685\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:51052\n",
            "INFO:distributed.scheduler:Receive client connection: Client-58fc12da-b4fe-11ef-91db-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:51066\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "DataFrame.dropna() got an unexpected keyword argument 'axis'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-466549fb6b13>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Drop columns with >80% missing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mdbnsfp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbnsfp_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Imputation for numeric columns with <20% missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: DataFrame.dropna() got an unexpected keyword argument 'axis'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# File path\n",
        "file_path = '/content/drive/MyDrive/filtered_dbnsfp.csv'\n",
        "chunk_size = 100000  # Define the chunk size\n",
        "\n",
        "#Missing Values list\n",
        "na_list = [\n",
        "    '#chr', 'pos(1-based)', 'ref', 'alt',\n",
        "    'SIFT_converted_rankscore', 'SIFT4G_converted_rankscore', 'Polyphen2_HDIV_rankscore',\n",
        "    'Polyphen2_HVAR_rankscore', 'LRT_converted_rankscore', 'MutationTaster_converted_rankscore',\n",
        "    'MutationAssessor_rankscore', 'FATHMM_converted_rankscore', 'PROVEAN_converted_rankscore',\n",
        "    'VEST4_rankscore', 'MetaSVM_rankscore', 'MetaLR_rankscore', 'MetaRNN_rankscore', 'M-CAP_rankscore',\n",
        "    'REVEL_rankscore', 'MutPred_rankscore', 'MVP_rankscore', 'gMVP_rankscore', 'MPC_rankscore',\n",
        "    'PrimateAI_rankscore', 'DEOGEN2_rankscore', 'BayesDel_addAF_rankscore', 'BayesDel_noAF_rankscore',\n",
        "    'ClinPred_rankscore', 'LIST-S2_rankscore', 'VARITY_R_rankscore', 'VARITY_ER_rankscore',\n",
        "    'VARITY_R_LOO_rankscore', 'VARITY_ER_LOO_rankscore', 'ESM1b_rankscore', 'EVE_rankscore',\n",
        "    'AlphaMissense_rankscore', 'PHACTboost_rankscore', 'MutFormer_rankscore', 'MutScore_rankscore',\n",
        "    'CADD_raw_rankscore', 'CADD_raw_rankscore_hg19', 'DANN_rankscore', 'fathmm-MKL_coding_rankscore',\n",
        "    'fathmm-XF_coding_rankscore', 'Eigen-raw_coding_rankscore', 'Eigen-PC-raw_coding_rankscore',\n",
        "    'GenoCanyon_rankscore', 'integrated_fitCons_rankscore', 'GM12878_fitCons_rankscore',\n",
        "    'H1-hESC_fitCons_rankscore', 'HUVEC_fitCons_rankscore', 'LINSIGHT_rankscore', 'GERP++_RS_rankscore',\n",
        "    'GERP_91_mammals_rankscore', 'phyloP100way_vertebrate_rankscore', 'phyloP470way_mammalian_rankscore',\n",
        "    'phyloP17way_primate_rankscore', 'phastCons100way_vertebrate_rankscore', 'phastCons470way_mammalian_rankscore',\n",
        "    'phastCons17way_primate_rankscore', 'SiPhy_29way_logOdds_rankscore', 'bStatistic_converted_rankscore', '.',''\n",
        "]\n",
        "\n",
        "# Initialize variables to aggregate results across chunks\n",
        "missing_values_summary = None\n",
        "data_cleaned_chunks = []\n",
        "outlier_counts_summary = {}\n",
        "\n",
        "# Define outlier detection function\n",
        "def detect_outliers_iqr(df, feature):\n",
        "    Q1 = df[feature].quantile(0.25)\n",
        "    Q3 = df[feature].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
        "    return outliers\n",
        "\n",
        "# Process the data chunk by chunk\n",
        "for chunk in pd.read_csv(file_path, na_values=na_list, chunksize=chunk_size):\n",
        "    print(f\"Processing chunk with shape: {chunk.shape}\")\n",
        "\n",
        "    # Set dtypes of index features to object\n",
        "    index_features = ['#chr', 'pos(1-based)', 'ref', 'alt']\n",
        "    for feature in index_features:\n",
        "        if feature in chunk.columns:\n",
        "            chunk[feature] = chunk[feature].astype('object')\n",
        "\n",
        "\n",
        "    # Separate non-index features for EDA\n",
        "    non_index_features = [col for col in chunk.columns if col not in index_features]\n",
        "    chunk_non_index = chunk[non_index_features]\n",
        "\n",
        "    # Count missing values per column in the chunk\n",
        "    missing_values_chunk = chunk_non_index.isnull().sum()\n",
        "\n",
        "    # Aggregate missing values across chunks\n",
        "    if missing_values_summary is None:\n",
        "        missing_values_summary = missing_values_chunk\n",
        "    else:\n",
        "        missing_values_summary += missing_values_chunk\n",
        "\n",
        "    # Remove features with >= 80% missing values\n",
        "    threshold = 0.8\n",
        "    features_to_keep = missing_values_chunk[missing_values_chunk < threshold * chunk.shape[0]].index\n",
        "    chunk_non_index = chunk_non_index[features_to_keep]\n",
        "\n",
        "    # Impute missing values for features with <20% missing values\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    chunk_imputed = pd.DataFrame(imputer.fit_transform(chunk_non_index), columns=chunk_non_index.columns)\n",
        "\n",
        "    # Reattach index features\n",
        "    chunk_imputed[index_features] = chunk[index_features].reset_index(drop=True)\n",
        "\n",
        "    # Detect and remove outliers for numeric columns\n",
        "    for feature in chunk_imputed.select_dtypes(include='number').columns:\n",
        "        Q1 = chunk_imputed[feature].quantile(0.25)\n",
        "        Q3 = chunk_imputed[feature].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        # Count outliers for reporting\n",
        "        outliers = chunk_imputed[(chunk_imputed[feature] < lower_bound) | (chunk_imputed[feature] > upper_bound)]\n",
        "        outlier_counts_summary[feature] = outlier_counts_summary.get(feature, 0) + len(outliers)\n",
        "\n",
        "        # Remove outliers\n",
        "        chunk_imputed = chunk_imputed[(chunk_imputed[feature] >= lower_bound) & (chunk_imputed[feature] <= upper_bound)]\n",
        "\n",
        "    # Collect the cleaned chunk\n",
        "    data_cleaned_chunks.append(chunk_imputed)\n",
        "\n",
        "# Combine all cleaned chunks\n",
        "cleaned_data = pd.concat(data_cleaned_chunks, ignore_index=True)\n",
        "\n",
        "# Report results\n",
        "removed_features = missing_values_summary[missing_values_summary >= threshold * chunk_size].index\n",
        "outlier_count_total = sum(outlier_counts_summary.values())\n",
        "\n",
        "report = []\n",
        "report.append(f\"Initial missing values per feature:\\n{missing_values_summary}\")\n",
        "report.append(f\"Number of features removed due to missing values: {len(removed_features)}\")\n",
        "report.append(f\"Outlier counts per feature:\\n{outlier_counts_summary}\")\n",
        "report.append(f\"Number of samples removed during outlier analysis: {outlier_count_total}\")\n",
        "report.append(f\"Final size of the cleaned dataset: {cleaned_data.shape}\")\n",
        "\n",
        "# Write report to a text file\n",
        "with open('dbnsfp_eda_results.txt', 'w') as report_file:\n",
        "    report_file.write(\"\\n\\n\".join(report))\n",
        "\n",
        "print(\"Data cleaning report saved to 'dbnsfp_eda_results.txt'\")\n",
        "\n",
        "# Visualize box plots\n",
        "features_with_outliers = [feature for feature, count in outlier_counts_summary.items() if count > 0][:2]\n",
        "features_without_outliers = [feature for feature, count in outlier_counts_summary.items() if count == 0][:2]\n",
        "\n",
        "# Plot box plots for features with outliers\n",
        "for feature in features_with_outliers:\n",
        "    sns.boxplot(x=cleaned_data[feature])\n",
        "    plt.title(f\"Box Plot for Feature with Outliers: {feature}\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot box plots for features without outliers\n",
        "for feature in features_without_outliers:\n",
        "    sns.boxplot(x=cleaned_data[feature])\n",
        "    plt.title(f\"Box Plot for Feature without Outliers: {feature}\")\n",
        "    plt.show()\n",
        "\n",
        "# Save the cleaned data\n",
        "cleaned_data.to_csv('cleaned_dbnsfp.csv', index=False)\n",
        "print(\"Cleaned data saved to 'cleaned_dbnsfp.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "09y_hYqVglDR",
        "outputId": "2ebf6d99-2f49-4c77-d0c3-b47de4348340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk with shape: (100000, 62)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8b697d0b298b>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Impute missing values for features with <20% missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mimputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mchunk_imputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_non_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_non_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Reattach index features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/impute/_knn.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess_chunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         )\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0;31m# process_chunk modifies X in place. No return value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2172\u001b[0;31m         \u001b[0mD_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2173\u001b[0m         if (X is Y or Y is None) and PAIRWISE_DISTANCE_FUNCTIONS.get(\n\u001b[1;32m   2174\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2373\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2375\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1893\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mnan_euclidean_distances\u001b[0;34m(X, Y, squared, missing_values, copy)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0mpresent_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmissing_X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0mpresent_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_X\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mmissing_Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m     \u001b[0mpresent_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresent_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresent_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m     \u001b[0mdistances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpresent_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;31m# avoid divide by zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}